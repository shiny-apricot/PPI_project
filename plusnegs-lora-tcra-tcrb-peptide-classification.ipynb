{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Table of contents**<a id='toc0_'></a>    \n- [Imports](#toc1_1_)    \n  - [Building dataset](#toc1_2_)    \n  - [Model training](#toc1_3_)    \n  - [Lora implementation](#toc1_4_)    \n\n<!-- vscode-jupyter-toc-config\n\tnumbering=false\n\tanchor=true\n\tflat=false\n\tminLevel=1\n\tmaxLevel=6\n\t/vscode-jupyter-toc-config -->\n<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->","metadata":{}},{"cell_type":"markdown","source":"# What is Done In This Notebook ?\n- Select the appropriate model and dataset.\n- Drop the unnecessary and NA columns.\n- Ignore the unnecessary string parts in the columns. (For ex: the last 140 characters in a column would always be the same. Then remove that part.)\n- Build the SequenceDataset class.\n- Build the evaluate function \n- Train the model","metadata":{}},{"cell_type":"markdown","source":"## <a id='toc1_1_'></a>[Imports](#toc0_)","metadata":{}},{"cell_type":"code","source":"!pip install focal_loss_torch\n!pip install peft==0.4.0\n\nimport torch\nfrom transformers import AutoTokenizer, EsmForSequenceClassification\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report, roc_auc_score\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import auc, precision_recall_curve, average_precision_score\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom accelerate import Accelerator\nfrom focal_loss.focal_loss import FocalLoss\nfrom torch.optim import AdamW\nfrom transformers import get_scheduler\nfrom peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n\nsns.set_theme()\nsns.set_context('paper')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-08-28T12:51:53.048098Z","iopub.execute_input":"2023-08-28T12:51:53.048457Z","iopub.status.idle":"2023-08-28T12:52:31.400016Z","shell.execute_reply.started":"2023-08-28T12:51:53.048430Z","shell.execute_reply":"2023-08-28T12:52:31.399007Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting focal_loss_torch\n  Downloading focal_loss_torch-0.1.2-py3-none-any.whl (4.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from focal_loss_torch) (2.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from focal_loss_torch) (1.23.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->focal_loss_torch) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->focal_loss_torch) (1.3.0)\nInstalling collected packages: focal_loss_torch\nSuccessfully installed focal_loss_torch-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting peft==0.4.0\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (5.4.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (4.29.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (0.12.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (0.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.14.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0) (2023.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\nInstalling collected packages: peft\nSuccessfully installed peft-0.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"cuda = torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:31.405527Z","iopub.execute_input":"2023-08-28T12:52:31.407841Z","iopub.status.idle":"2023-08-28T12:52:31.444823Z","shell.execute_reply.started":"2023-08-28T12:52:31.407804Z","shell.execute_reply":"2023-08-28T12:52:31.443630Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:31.446196Z","iopub.execute_input":"2023-08-28T12:52:31.446616Z","iopub.status.idle":"2023-08-28T12:52:31.456603Z","shell.execute_reply.started":"2023-08-28T12:52:31.446583Z","shell.execute_reply":"2023-08-28T12:52:31.455655Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Parameters","metadata":{}},{"cell_type":"code","source":"experimental = False\n# if you set 'experimental' to True, then make this 1 to prevent confusion\nexperiment_on = 1 # example: 0.01\ndata = 'plusnegs'\n\nmodel_tag = '8m' # options are 8m, 35m, 150m\n\n# Feel free to play around with these hyper-parameters\nlr = 5e-5\nnum_epochs = 3\n\n# This is somewhat important, as sequences that are longer than this just get truncated to this length. \n# For very long sequences, this can render the model useless. In our experience, this generally still works fine. Notice that memory requirements grow dramatically with max_length!\n# max_length = 1028\nmax_length = 630\n\nif model_tag == '8m':\n    model_name = \"facebook/esm2_t6_8M_UR50D\"\nelif model_tag == '35m':\n    model_name = \"facebook/esm2_t12_35M_UR50D\"\nelif model_tag == '150m':\n    model_name = \"facebook/esm2_t30_150M_UR50D\"\nelse:\n    raise ValueError(\"Invalid model tag\")","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:31.459695Z","iopub.execute_input":"2023-08-28T12:52:31.460524Z","iopub.status.idle":"2023-08-28T12:52:31.470862Z","shell.execute_reply.started":"2023-08-28T12:52:31.460491Z","shell.execute_reply":"2023-08-28T12:52:31.470019Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if data == 'plusnegs':\n    train_path = Path('/kaggle/input/plusnegswholedata/traintest_plusnegs_plm.csv')\n    test_path = Path('/kaggle/input/plusnegswholedata/indep_plusnegs_plm.csv')\n    \nif data == 'ds_complex-antigen':\n    train_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_train_only-antigen.csv')\n    test_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_test_only-antigen.csv')\nif data == 'ds_complex-TRA':\n    train_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_train_only-tra.csv')\n    test_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_test_only-tra.csv')\nif data == 'ds_complex-TRB':\n    train_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_train_only-trb.csv')\n    test_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_test_only-trb.csv')\n\nif data == 'plusnegs_antigen':\n    train_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/traintest_plusnegs_plm_only_antigen.csv')\n    test_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/indep_plusnegs_plm_only_antigen.csv')\nif data == 'plusnegs_TRA':\n    train_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/traintest_plusnegs_plm_only_TRA.csv')\n    test_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/indep_plusnegs_plm_only_TRA.csv')\nif data == 'plusnegs_TRB':\n    train_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/traintest_plusnegs_plm_only_TRB.csv')\n    test_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/indep_plusnegs_plm_only_TRB.csv')\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:31.473269Z","iopub.execute_input":"2023-08-28T12:52:31.474179Z","iopub.status.idle":"2023-08-28T12:52:31.490664Z","shell.execute_reply.started":"2023-08-28T12:52:31.474139Z","shell.execute_reply":"2023-08-28T12:52:31.489800Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(train_path)\ndf_test = pd.read_csv(test_path)\ndisplay( df_train.head(1))\n\n# drop unnecessary columns\ndf_train = df_train.drop(columns=['COMPLEX_ID'])\ndf_test = df_test.drop(columns=['COMPLEX_ID'])\ndisplay( df_train.head(1) )\nprint( df_train.shape )\n\n# drop null columns\nprint('after drop null columns')\ndf_train = df_train.dropna()\ndf_test = df_test.dropna()\nprint(df_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:31.493276Z","iopub.execute_input":"2023-08-28T12:52:31.494900Z","iopub.status.idle":"2023-08-28T12:52:32.646325Z","shell.execute_reply.started":"2023-08-28T12:52:31.494869Z","shell.execute_reply":"2023-08-28T12:52:32.645209Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"                   COMPLEX_ID antigen.epitope  \\\n0  pos_vdj_plm_train_tcr_ep_1     KAFSPEVIPMF   \n\n                                              TRA_aa  \\\n0  MLLLLVPVLEVIFTLGGTRAQSVTQLGSHVSVSEGALVLLRCNYSS...   \n\n                                              TRB_aa  tcr_affinity  \n0  MSNQVLCCVVLCFLGANTVDGGITQSPKYLFRKEGQNVTLSCEQNL...             1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>COMPLEX_ID</th>\n      <th>antigen.epitope</th>\n      <th>TRA_aa</th>\n      <th>TRB_aa</th>\n      <th>tcr_affinity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pos_vdj_plm_train_tcr_ep_1</td>\n      <td>KAFSPEVIPMF</td>\n      <td>MLLLLVPVLEVIFTLGGTRAQSVTQLGSHVSVSEGALVLLRCNYSS...</td>\n      <td>MSNQVLCCVVLCFLGANTVDGGITQSPKYLFRKEGQNVTLSCEQNL...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  antigen.epitope                                             TRA_aa  \\\n0     KAFSPEVIPMF  MLLLLVPVLEVIFTLGGTRAQSVTQLGSHVSVSEGALVLLRCNYSS...   \n\n                                              TRB_aa  tcr_affinity  \n0  MSNQVLCCVVLCFLGANTVDGGITQSPKYLFRKEGQNVTLSCEQNL...             1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>antigen.epitope</th>\n      <th>TRA_aa</th>\n      <th>TRB_aa</th>\n      <th>tcr_affinity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KAFSPEVIPMF</td>\n      <td>MLLLLVPVLEVIFTLGGTRAQSVTQLGSHVSVSEGALVLLRCNYSS...</td>\n      <td>MSNQVLCCVVLCFLGANTVDGGITQSPKYLFRKEGQNVTLSCEQNL...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"(38536, 4)\nafter drop null columns\n(38528, 4)\n","output_type":"stream"}]},{"cell_type":"code","source":"if experimental:\n    df_train = df_train.sample(n=int(df_train.shape[0] * experiment_on), random_state=44).reset_index(drop=True)\nif experimental:\n    df_test = df_test.sample(n=int(df_test.shape[0] * experiment_on), random_state=44).reset_index(drop=True)\n\nprint('the shape after sampling')\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:32.650861Z","iopub.execute_input":"2023-08-28T12:52:32.653073Z","iopub.status.idle":"2023-08-28T12:52:32.667640Z","shell.execute_reply.started":"2023-08-28T12:52:32.653039Z","shell.execute_reply":"2023-08-28T12:52:32.666738Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"the shape after sampling\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(38528, 4)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"markdown","source":"> Calculate the length of the longest string in each column. ","metadata":{}},{"cell_type":"code","source":"max_antigen_length = df_train['antigen.epitope'].apply(len).max()\nmax_TRA_length = df_train['TRA_aa'].apply(len).max()\nmax_TRB_length = df_train['TRB_aa'].apply(len).max()\n\nmax_antigen_length, max_TRA_length, max_TRB_length","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:32.672239Z","iopub.execute_input":"2023-08-28T12:52:32.675057Z","iopub.status.idle":"2023-08-28T12:52:32.780089Z","shell.execute_reply.started":"2023-08-28T12:52:32.675016Z","shell.execute_reply":"2023-08-28T12:52:32.779028Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(13, 285, 331)"},"metadata":{}}]},{"cell_type":"code","source":"df_train['TRA_aa'].str[-140:].unique().shape","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:32.784908Z","iopub.execute_input":"2023-08-28T12:52:32.787528Z","iopub.status.idle":"2023-08-28T12:52:32.846998Z","shell.execute_reply.started":"2023-08-28T12:52:32.787491Z","shell.execute_reply":"2023-08-28T12:52:32.846108Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(1,)"},"metadata":{}}]},{"cell_type":"markdown","source":"> The last 140 characters of TRA_aa column is **always** the same. So we will ignore them.","metadata":{}},{"cell_type":"code","source":"df_train['TRA_aa'] = df_train['TRA_aa'].str[-140:]","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:32.854134Z","iopub.execute_input":"2023-08-28T12:52:32.856365Z","iopub.status.idle":"2023-08-28T12:52:32.894900Z","shell.execute_reply.started":"2023-08-28T12:52:32.856332Z","shell.execute_reply":"2023-08-28T12:52:32.893981Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"max_antigen_length = df_train['antigen.epitope'].apply(len).max()\nmax_TRA_length = df_train['TRA_aa'].apply(len).max()\nmax_TRB_length = df_train['TRB_aa'].apply(len).max()\n\nmax_antigen_length, max_TRA_length, max_TRB_length","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:32.899827Z","iopub.execute_input":"2023-08-28T12:52:32.902430Z","iopub.status.idle":"2023-08-28T12:52:33.004134Z","shell.execute_reply.started":"2023-08-28T12:52:32.902392Z","shell.execute_reply":"2023-08-28T12:52:33.003207Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(13, 140, 331)"},"metadata":{}}]},{"cell_type":"markdown","source":"> Now the length of TRA_aa is halved.","metadata":{}},{"cell_type":"markdown","source":"## <a id='toc1_2_'></a>[Building dataset](#toc0_)","metadata":{}},{"cell_type":"code","source":"df_train.columns","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:33.008435Z","iopub.execute_input":"2023-08-28T12:52:33.010657Z","iopub.status.idle":"2023-08-28T12:52:33.020570Z","shell.execute_reply.started":"2023-08-28T12:52:33.010625Z","shell.execute_reply":"2023-08-28T12:52:33.019688Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Index(['antigen.epitope', 'TRA_aa', 'TRB_aa', 'tcr_affinity'], dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"First, we need to construct our data set. Since we are training a binary sequence classifier, we need positive and negative examples. \n\nWe simply split our data into a train/test split and place the positive and negative example sequences into separate text files.","metadata":{}},{"cell_type":"code","source":"sequence_field = 3\nclass SequenceDataset(torch.utils.data.Dataset):\n    def __init__(self, df, plot=False):\n        self.data = df\n        \n        if plot:\n            plt.hist([len(s[0]) for s in self.data], bins=64)\n            plt.show()\n            \n        if data[:8] == 'plusnegs': # if dataset is plusnegs\n            self.data_processed = pd.DataFrame()\n            self.data_processed['x'] = self.data['antigen.epitope'] + '<eos>' + self.data['TRA_aa'] + '<eos>' + self.data['TRB_aa']\n            self.data_processed['y'] = self.data['tcr_affinity']\n        print(f\"Initialized dataset consisting of {self.data.shape[0]} sequences.\")\n        \n    def __getitem__(self, idx):\n#         x = self.data.columns[:-1]\n#         y = self.data.columns[-1]\n#         return self.data.loc[idx, x].tolist(), self.data.loc[idx, y].tolist()\n        data = self.data_processed.iloc[idx].tolist()\n        return data\n    \n    def __len__(self):\n        return self.data.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:33.025146Z","iopub.execute_input":"2023-08-28T12:52:33.027451Z","iopub.status.idle":"2023-08-28T12:52:33.041871Z","shell.execute_reply.started":"2023-08-28T12:52:33.027419Z","shell.execute_reply":"2023-08-28T12:52:33.040844Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train = SequenceDataset(df_train)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:33.043253Z","iopub.execute_input":"2023-08-28T12:52:33.043998Z","iopub.status.idle":"2023-08-28T12:52:33.114896Z","shell.execute_reply.started":"2023-08-28T12:52:33.043945Z","shell.execute_reply":"2023-08-28T12:52:33.113935Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Initialized dataset consisting of 38528 sequences.\n","output_type":"stream"}]},{"cell_type":"code","source":"test = SequenceDataset(df_test)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:33.119284Z","iopub.execute_input":"2023-08-28T12:52:33.121572Z","iopub.status.idle":"2023-08-28T12:52:33.136046Z","shell.execute_reply.started":"2023-08-28T12:52:33.121538Z","shell.execute_reply":"2023-08-28T12:52:33.134822Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Initialized dataset consisting of 1498 sequences.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Loader","metadata":{}},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(train, batch_size=4, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=4, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:33.140696Z","iopub.execute_input":"2023-08-28T12:52:33.142993Z","iopub.status.idle":"2023-08-28T12:52:33.150771Z","shell.execute_reply.started":"2023-08-28T12:52:33.142945Z","shell.execute_reply":"2023-08-28T12:52:33.149274Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for s,l in train_loader:\n    print(s)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:33.155610Z","iopub.execute_input":"2023-08-28T12:52:33.158118Z","iopub.status.idle":"2023-08-28T12:52:33.208241Z","shell.execute_reply.started":"2023-08-28T12:52:33.158085Z","shell.execute_reply":"2023-08-28T12:52:33.207360Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"('KLGGALQAK<eos>IQNPDPAVYQLRDSKSSDKSVCLFTDFDSQTNVSQSKDSDVYITDKTVLDMRSMDFKSNSAVAWSNKSDFACANAFNNSIIPEDTFFPSPESSCDVKLVEKSFETDTNLNFQNLSVIGFRILLLKVAGFNLLMTLRLWSS<eos>MDTWLVCWAIFSLLKAGLTEPEVTQTPSHQVTQMGQEVILRCVPISNHLYFYWYRQILGQKVEFLVSFYNNEISEKSEIFDDQFSVERPDGSNFTLKIRSTKLEDSAMYFCASSEVGVFEAFFGQGTRLTVVEDLNKVFPPEVAVFEPSEAEISHTQKATLVCLATGFFPDHVELSWWVNGKEVHSGVSTDPQPLKEQPALNDSRYCLSSRLRVSATFWQNPRNHFRCQVQFYGLSENDEWTQDRAKPVTQIVSAEAWGRADCGFTSVSYQQGVLSATILYEILLGKATLYAVLVSALVLMAMVKRKDF', 'AVFDRKSDAK<eos>IQNPDPAVYQLRDSKSSDKSVCLFTDFDSQTNVSQSKDSDVYITDKTVLDMRSMDFKSNSAVAWSNKSDFACANAFNNSIIPEDTFFPSPESSCDVKLVEKSFETDTNLNFQNLSVIGFRILLLKVAGFNLLMTLRLWSS<eos>MGPQLLGYVVLCLLGAGPLEAQVTQNPRYLITVTGKKLTVTCSQNMNHEYMSWYRQDPGLGLRQIYYSMNVEVTDKGDVPEGYKVSRKEKRNFPLILESPSPNQTSLYFCASSRTGWGSGNTIYFGEGSWLTVVEDLNKVFPPEVAVFEPSEAEISHTQKATLVCLATGFFPDHVELSWWVNGKEVHSGVSTDPQPLKEQPALNDSRYCLSSRLRVSATFWQNPRNHFRCQVQFYGLSENDEWTQDRAKPVTQIVSAEAWGRADCGFTSVSYQQGVLSATILYEILLGKATLYAVLVSALVLMAMVKRKDF', 'KLGGALQAK<eos>IQNPDPAVYQLRDSKSSDKSVCLFTDFDSQTNVSQSKDSDVYITDKTVLDMRSMDFKSNSAVAWSNKSDFACANAFNNSIIPEDTFFPSPESSCDVKLVEKSFETDTNLNFQNLSVIGFRILLLKVAGFNLLMTLRLWSS<eos>MGIRLLCRVAFCFLAVGLVDVKVTQSSRYLVKRTGEKVFLECVQDMDHENMFWYRQDPGLGLRLIYFSYDVKMKEKGDIPEGYSVSREKKERFSLILESASTNQTSMYLCASSLSGQRVSGNTIYFGEGSWLTVVEDLNKVFPPEVAVFEPSEAEISHTQKATLVCLATGFFPDHVELSWWVNGKEVHSGVSTDPQPLKEQPALNDSRYCLSSRLRVSATFWQNPRNHFRCQVQFYGLSENDEWTQDRAKPVTQIVSAEAWGRADCGFTSVSYQQGVLSATILYEILLGKATLYAVLVSALVLMAMVKRKDF', 'NYNYLYRLF<eos>IQNPDPAVYQLRDSKSSDKSVCLFTDFDSQTNVSQSKDSDVYITDKTVLDMRSMDFKSNSAVAWSNKSDFACANAFNNSIIPEDTFFPSPESSCDVKLVEKSFETDTNLNFQNLSVIGFRILLLKVAGFNLLMTLRLWSS<eos>MGCRLLCCAVLCLLGAVPIDTEVTQTPKHLVMGMTNKKSLKCEQHMGHRAMYWYKQKAKKPPELMFVYSYEKLSINESVPSRFSPECPNSSLLNLHLHALQPEDSALYLCASSHSQGATGELFFGEGSRLTVLEDLKNVFPPEVAVFEPSEAEISHTQKATLVCLATGFYPDHVELSWWVNGKEVHSGVSTDPQPLKEQPALNDSRYCLSSRLRVSATFWQNPRNHFRCQVQFYGLSENDEWTQDRAKPVTQIVSAEAWGRADCGFTSESYQQGVLSATILYEILLGKATLYAVLVSALVLMAMVKRKDSRG')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## <a id='toc1_3_'></a>[Model training](#toc0_)","metadata":{}},{"cell_type":"markdown","source":"We are using the [ESM-2 model](https://huggingface.co/docs/transformers/model_doc/esm) by Meta.\n","metadata":{}},{"cell_type":"code","source":"# Initialize pre-trained model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = EsmForSequenceClassification.from_pretrained(model_name)\n\nif cuda:\n    model = model.cuda()\n\noptimizer = AdamW(model.parameters(), lr=lr)\n\n# A linear lr schedule worked well in our experiments. \nnum_training_steps = num_epochs * len(train_loader)\nlr_scheduler = get_scheduler(\n     name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n\n# Replacing the standard cross-entropy loss by a focal loss improved our results slightly.\nfocal = FocalLoss(gamma=0.7)\n\nif cuda:\n    accelerator = Accelerator(mixed_precision=\"fp16\") # Change to be able to run in old kaggle gpus\nelse:\n    accelerator = Accelerator(mixed_precision=\"bf16\")\n\n    \nmodel, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:52:33.212404Z","iopub.execute_input":"2023-08-28T12:52:33.214642Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fed759aa4c8742a7afa01bd963454964"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abd7d221c4124d12bb151ee0c9e2465e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a679c7643bc4172b69e05aeb9a75908"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f66be17aea14a208400b593843d65c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/31.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"593bf9bef0cf4717a46e9e950b05483b"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Given a trained model, we'd like to evaluate how well we are doing.\n\nMake a prediction for every example in the test set and report total accuracy (only makes sense if our test set is balanced!) and a calibration curve.","metadata":{}},{"cell_type":"code","source":"def validate(model):\n    model.eval()\n    \n    targets = []\n    probabilities = []\n    predictions = []\n    \n    for s, l in tqdm(test_loader):\n        \n        inputs = tokenizer(s, return_tensors='pt', padding=\"max_length\", truncation=True,\n                           max_length=max_length)\n        if cuda:\n            inputs = inputs.to(\"cuda\")\n        \n        with torch.no_grad():\n            logits = model(**inputs, labels=l).logits\n            probs = torch.softmax(logits, dim=1)\n            preds = torch.argmax(logits, dim=1)\n            \n        predictions.append(preds)    \n        probabilities.append(probs)\n        targets.append(l)\n            \n    targets = torch.cat(targets).cpu()\n    probabilities = torch.cat(probabilities).cpu()\n    predictions = torch.cat(predictions).cpu()\n            \n    # Visualize calibration curve\n    # prob_true, prob_pred = calibration_curve(targets, probabilities[:,1], pos_label=None, n_bins=10, strategy='uniform')\n    # sns.lineplot(x=prob_true, y=prob_pred)\n    # sns.lineplot(x=np.linspace(0,1,5), y=np.linspace(0,1,5))\n    # plt.show()\n    \n    print(\"Test accuracy:\", accuracy_score(targets, predictions))\n    print(\"Test precision:\", precision_score(targets, predictions))\n    print(\"Test recall:\", recall_score(targets, predictions))\n    print('Confusion matrix:\\n', confusion_matrix(targets, predictions))\n    \n    # Reset our model to trraining mode before exiting evaluation, \n    # so we don't forget to do this later!\n    model.train()\n    \n    return(targets, probabilities, predictions)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Now, we simply train our model and see how it performs.","metadata":{}},{"cell_type":"code","source":"num_training_steps","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nprogress_bar = tqdm(range(num_training_steps))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor epoch in range(num_epochs):\n    print('epoch:', epoch)\n    model.train()\n     \n    for idx, (s, l) in enumerate(train_loader):\n        \n        inputs = tokenizer(s, return_tensors='pt', padding=\"max_length\", truncation=True, \n                           max_length=max_length)\n        if cuda:\n            inputs = inputs.to(\"cuda\")\n        logits = model(**inputs, labels=l).logits\n        \n        if cuda:\n            loss = focal(torch.softmax(logits, dim=1), l.cuda())\n        else:\n            loss = focal(torch.softmax(logits, dim=1), l)\n        \n        accelerator.backward(loss)\n        lr_scheduler.step()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        progress_bar.update(1)\n          \n    validate(model)\n    \n    print('PARAMETERS:')\n    print('experimental:', experimental)\n    print('experiment_on:', experiment_on)\n    print('data:', data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained('./lora_tra_trb_training_peptide_all_dataset_eos')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='toc1_4_'></a>[Lora implementation](#toc0_)","metadata":{}},{"cell_type":"markdown","source":"For larger models, you might have to do parameter efficient fine-tuning. Here, we quickly demonstrate this using a technique called [LoRA](https://arxiv.org/abs/2106.09685).","metadata":{}},{"cell_type":"code","source":"model_name = \"facebook/esm2_t12_35M_UR50D\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = EsmForSequenceClassification.from_pretrained(model_name).cuda()\n\n#\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS, inference_mode=False, r=8, lora_alpha=32, \n    lora_dropout=0.1, bias=\"none\", target_modules=[\"query\", \"value\"], \n    modules_to_save=[\"decode_head\"], # make sure to set classification heads here to save so we do train them\n)\n\nlora_model = get_peft_model(model, peft_config)\nlora_model.print_trainable_parameters()\n\noptimizer = AdamW(lora_model.parameters(), lr=lr)\nnum_training_steps = num_epochs * len(train_loader)\nlr_scheduler = get_scheduler(\n     name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n\nprogress_bar = tqdm(range(num_training_steps))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    lora_model.train()\n    \n    for s, l in train_loader:\n        \n        inputs = tokenizer(s, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=max_length).to(\"cuda\")\n        logits = lora_model(**inputs, labels=l).logits\n        \n        loss = focal(torch.softmax(logits, dim=1), l.cuda())\n        \n        loss.backward()\n        lr_scheduler.step()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        progress_bar.update(1)\n        \n    validate(lora_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results with Lora and a larger model do not get better, but maybe increasing the number of epochs would do the trick. Actually, the test accuracy keeps improving.","metadata":{}},{"cell_type":"markdown","source":"## Results:\ndata: plusnegs with only antigen column\nmodel: 8M\n\n    Test accuracy: 0.07142857142857142\n    Test precision: 0.07142857142857142\n    Test recall: 1.0\n    Confusion matrix:\n     [[   0 1872]\n     [   0  144]]\n\ndata: plusnegs with only TRA columns (1/4 sample)\nmodel: plusnegs\n\n    Test accuracy: 0.07936507936507936\n    Test precision: 0.07142857142857142\n    Test recall: 0.9907407407407407\n    Confusion matrix:\n     [[  13 1391]\n     [   1  107]]\n\ndata: plusnegs with only TRB columns (1/4 sample)\nmodel: plusnegs\n\n    Test accuracy: 0.07142857142857142\n    Test precision: 0.07142857142857142\n    Test recall: 1.0\n    Confusion matrix:\n     [[   0 1859]\n     [   0  143]]\n\n     \ndata: ds-complex with only antigen column (1/100 sample)\nmodel: 8M\n\n    Test accuracy: 0.9055211803902904\n    Test precision: 0.9307381193124368\n    Test recall: 0.8762494050452165\n    Confusion matrix:\n     [[1964  137]\n     [ 260 1841]]\ndata: ds-complex with only TRA column (1/100 sample)\nmodel: 8M\n\n    Test accuracy: 0.5\n    Test precision: 0.5\n    Test recall: 1.0\n    Confusion matrix:\n     [[   0 2101]\n     [   0 2101]]\ndata: ds-complex with only TRB column (1/100 sample)\nmodel: 8M\n\n    Test accuracy: 0.5\n    Test precision: 0.5\n    Test recall: 1.0\n    Confusion matrix:\n     [[   0 2101]\n     [   0 2101]]\n     ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}