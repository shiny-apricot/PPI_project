{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Table of contents**<a id='toc0_'></a>    \n- [Imports](#toc1_1_)    \n  - [Building dataset](#toc1_2_)    \n  - [Model training](#toc1_3_)    \n  - [Lora implementation](#toc1_4_)    \n\n<!-- vscode-jupyter-toc-config\n\tnumbering=false\n\tanchor=true\n\tflat=false\n\tminLevel=1\n\tmaxLevel=6\n\t/vscode-jupyter-toc-config -->\n<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->","metadata":{}},{"cell_type":"markdown","source":"# What is Done In This Notebook ?\n- Select the appropriate model and dataset.\n- Drop the unnecessary and NA columns.\n- Ignore the unnecessary string parts in the columns. (For ex: the last 140 characters in a column would always be the same. Then remove that part.)\n- Build the SequenceDataset class.\n- Build the evaluate function \n- Train the model using accelerate library.","metadata":{}},{"cell_type":"markdown","source":"## <a id='toc1_1_'></a>[Imports](#toc0_)","metadata":{}},{"cell_type":"code","source":"# import os\n# from accelerate.utils import write_basic_config\n\n# write_basic_config()  # Write a config file\n# os._exit(00)  # Restart the notebook","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:00:59.200528Z","iopub.execute_input":"2023-08-29T12:00:59.201192Z","iopub.status.idle":"2023-08-29T12:00:59.206106Z","shell.execute_reply.started":"2023-08-29T12:00:59.201157Z","shell.execute_reply":"2023-08-29T12:00:59.205197Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install focal_loss_torch\n# !pip install peft==0.4.0\n!pip install accelerate==0.22.0","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:00:59.216600Z","iopub.execute_input":"2023-08-29T12:00:59.217600Z","iopub.status.idle":"2023-08-29T12:01:25.646879Z","shell.execute_reply.started":"2023-08-29T12:00:59.217566Z","shell.execute_reply":"2023-08-29T12:01:25.645725Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting focal_loss_torch\n  Downloading focal_loss_torch-0.1.2-py3-none-any.whl (4.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from focal_loss_torch) (2.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from focal_loss_torch) (1.23.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->focal_loss_torch) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->focal_loss_torch) (1.3.0)\nInstalling collected packages: focal_loss_torch\nSuccessfully installed focal_loss_torch-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting accelerate==0.22.0\n  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (5.4.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.22.0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.22.0) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.12.0\n    Uninstalling accelerate-0.12.0:\n      Successfully uninstalled accelerate-0.12.0\nSuccessfully installed accelerate-0.22.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, EsmForSequenceClassification\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report, roc_auc_score\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import auc, precision_recall_curve, average_precision_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom focal_loss.focal_loss import FocalLoss\nfrom torch.optim import AdamW\nfrom transformers import get_scheduler\nimport datetime\n\n#LoRA\n# from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n\n# Accelerate parts\nfrom accelerate import Accelerator, notebook_launcher # main interface, distributed launcher\nfrom accelerate.utils import set_seed # reproducability across devices\n\nsns.set_theme()\nsns.set_context('paper')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-08-29T12:01:25.649383Z","iopub.execute_input":"2023-08-29T12:01:25.649784Z","iopub.status.idle":"2023-08-29T12:01:49.532323Z","shell.execute_reply.started":"2023-08-29T12:01:25.649725Z","shell.execute_reply":"2023-08-29T12:01:49.531345Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# cuda = torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:49.534000Z","iopub.execute_input":"2023-08-29T12:01:49.534363Z","iopub.status.idle":"2023-08-29T12:01:49.541070Z","shell.execute_reply.started":"2023-08-29T12:01:49.534331Z","shell.execute_reply":"2023-08-29T12:01:49.540167Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:49.544314Z","iopub.execute_input":"2023-08-29T12:01:49.544724Z","iopub.status.idle":"2023-08-29T12:01:49.551166Z","shell.execute_reply.started":"2023-08-29T12:01:49.544687Z","shell.execute_reply":"2023-08-29T12:01:49.549723Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Parameters","metadata":{}},{"cell_type":"code","source":"experimental = False\nexperiment_on = 0.005 # example: 0.01\ndata = 'plusnegs'\nmodel_tag = '35m' # options are 8m, 35m, 150m\n\n# Feel free to play around with these hyper-parameters\nlr = 5e-5\nnum_epochs = 5\n\n# This is somewhat important, as sequences that are longer than this just get truncated to this length. \n# For very long sequences, this can render the model useless. In our experience, this generally still works fine. Notice that memory requirements grow dramatically with max_length!\n# max_length = 1028\nmax_length = 630\n\nif model_tag == '8m':\n    model_name = \"facebook/esm2_t6_8M_UR50D\"\nelif model_tag == '35m':\n    model_name = \"facebook/esm2_t12_35M_UR50D\"\nelif model_tag == '150m':\n    model_name = \"facebook/esm2_t30_150M_UR50D\"\nelse:\n    raise ValueError(\"Invalid model tag\")","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:49.552736Z","iopub.execute_input":"2023-08-29T12:01:49.553245Z","iopub.status.idle":"2023-08-29T12:01:49.563572Z","shell.execute_reply.started":"2023-08-29T12:01:49.553205Z","shell.execute_reply":"2023-08-29T12:01:49.562591Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if data == 'plusnegs':\n    train_path = Path('/kaggle/input/plusnegswholedata/traintest_plusnegs_plm.csv')\n    test_path = Path('/kaggle/input/plusnegswholedata/indep_plusnegs_plm.csv')\n    \nif data == 'ds_complex-antigen':\n    train_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_train_only-antigen.csv')\n    test_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_test_only-antigen.csv')\nif data == 'ds_complex-TRA':\n    train_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_train_only-tra.csv')\n    test_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_test_only-tra.csv')\nif data == 'ds_complex-TRB':\n    train_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_train_only-trb.csv')\n    test_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_test_only-trb.csv')\n\nif data == 'plusnegs_antigen':\n    train_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/traintest_plusnegs_plm_only_antigen.csv')\n    test_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/indep_plusnegs_plm_only_antigen.csv')\nif data == 'plusnegs_TRA':\n    train_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/traintest_plusnegs_plm_only_TRA.csv')\n    test_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/indep_plusnegs_plm_only_TRA.csv')\nif data == 'plusnegs_TRB':\n    train_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/traintest_plusnegs_plm_only_TRB.csv')\n    test_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/indep_plusnegs_plm_only_TRB.csv')\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:49.564981Z","iopub.execute_input":"2023-08-29T12:01:49.565549Z","iopub.status.idle":"2023-08-29T12:01:49.580465Z","shell.execute_reply.started":"2023-08-29T12:01:49.565513Z","shell.execute_reply":"2023-08-29T12:01:49.579626Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(train_path)\ndf_test = pd.read_csv(test_path)\ndisplay( df_train.head(1))\n\n# drop unnecessary columns\ndf_train = df_train.drop(columns=['COMPLEX_ID'])\ndf_test = df_test.drop(columns=['COMPLEX_ID'])\ndisplay( df_train.head(1) )\nprint( df_train.shape )\n\n# drop null columns\nprint('after drop null columns')\ndf_train = df_train.dropna()\ndf_test = df_test.dropna()\nprint(df_train.shape)\n\nif experimental:\n    df_train = df_train.sample(n=int(df_train.shape[0] * experiment_on), random_state=44).reset_index(drop=True)\nif experimental:\n    df_test = df_test.sample(n=int(df_test.shape[0] * experiment_on), random_state=44).reset_index(drop=True)\n\nprint('the shape after sampling')\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:49.582390Z","iopub.execute_input":"2023-08-29T12:01:49.582765Z","iopub.status.idle":"2023-08-29T12:01:50.174011Z","shell.execute_reply.started":"2023-08-29T12:01:49.582733Z","shell.execute_reply":"2023-08-29T12:01:50.172924Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"                   COMPLEX_ID antigen.epitope  \\\n0  pos_vdj_plm_train_tcr_ep_1     KAFSPEVIPMF   \n\n                                              TRA_aa  \\\n0  MLLLLVPVLEVIFTLGGTRAQSVTQLGSHVSVSEGALVLLRCNYSS...   \n\n                                              TRB_aa  tcr_affinity  \n0  MSNQVLCCVVLCFLGANTVDGGITQSPKYLFRKEGQNVTLSCEQNL...             1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>COMPLEX_ID</th>\n      <th>antigen.epitope</th>\n      <th>TRA_aa</th>\n      <th>TRB_aa</th>\n      <th>tcr_affinity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pos_vdj_plm_train_tcr_ep_1</td>\n      <td>KAFSPEVIPMF</td>\n      <td>MLLLLVPVLEVIFTLGGTRAQSVTQLGSHVSVSEGALVLLRCNYSS...</td>\n      <td>MSNQVLCCVVLCFLGANTVDGGITQSPKYLFRKEGQNVTLSCEQNL...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  antigen.epitope                                             TRA_aa  \\\n0     KAFSPEVIPMF  MLLLLVPVLEVIFTLGGTRAQSVTQLGSHVSVSEGALVLLRCNYSS...   \n\n                                              TRB_aa  tcr_affinity  \n0  MSNQVLCCVVLCFLGANTVDGGITQSPKYLFRKEGQNVTLSCEQNL...             1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>antigen.epitope</th>\n      <th>TRA_aa</th>\n      <th>TRB_aa</th>\n      <th>tcr_affinity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KAFSPEVIPMF</td>\n      <td>MLLLLVPVLEVIFTLGGTRAQSVTQLGSHVSVSEGALVLLRCNYSS...</td>\n      <td>MSNQVLCCVVLCFLGANTVDGGITQSPKYLFRKEGQNVTLSCEQNL...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"(38536, 4)\nafter drop null columns\n(38528, 4)\nthe shape after sampling\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(38528, 4)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"markdown","source":"> Calculate the length of the longest string in each column. ","metadata":{}},{"cell_type":"code","source":"max_antigen_length = df_train['antigen.epitope'].apply(len).max()\nmax_TRA_length = df_train['TRA_aa'].apply(len).max()\nmax_TRB_length = df_train['TRB_aa'].apply(len).max()\n\nmax_antigen_length, max_TRA_length, max_TRB_length","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:50.175543Z","iopub.execute_input":"2023-08-29T12:01:50.176134Z","iopub.status.idle":"2023-08-29T12:01:50.245805Z","shell.execute_reply.started":"2023-08-29T12:01:50.176100Z","shell.execute_reply":"2023-08-29T12:01:50.244675Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(13, 285, 331)"},"metadata":{}}]},{"cell_type":"code","source":"df_train['TRA_aa'].str[-140:].unique().shape, df_test['TRA_aa'].str[-140:].unique().shape","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:50.248411Z","iopub.execute_input":"2023-08-29T12:01:50.249196Z","iopub.status.idle":"2023-08-29T12:01:50.293533Z","shell.execute_reply.started":"2023-08-29T12:01:50.249161Z","shell.execute_reply":"2023-08-29T12:01:50.292551Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"((1,), (1,))"},"metadata":{}}]},{"cell_type":"markdown","source":"> The last 140 characters of TRA_aa column is **always** the same. So we will ignore them.","metadata":{}},{"cell_type":"code","source":"df_train['TRA_aa'] = df_train['TRA_aa'].str[-140:]\ndf_test['TRA_aa'] = df_test['TRA_aa'].str[-140:]","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:50.297982Z","iopub.execute_input":"2023-08-29T12:01:50.298579Z","iopub.status.idle":"2023-08-29T12:01:50.334697Z","shell.execute_reply.started":"2023-08-29T12:01:50.298548Z","shell.execute_reply":"2023-08-29T12:01:50.333632Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"max_antigen_length = df_train['antigen.epitope'].apply(len).max()\nmax_TRA_length = df_train['TRA_aa'].apply(len).max()\nmax_TRB_length = df_train['TRB_aa'].apply(len).max()\n\nmax_antigen_length, max_TRA_length, max_TRB_length","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:50.335996Z","iopub.execute_input":"2023-08-29T12:01:50.336816Z","iopub.status.idle":"2023-08-29T12:01:50.409842Z","shell.execute_reply.started":"2023-08-29T12:01:50.336784Z","shell.execute_reply":"2023-08-29T12:01:50.408408Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(13, 140, 331)"},"metadata":{}}]},{"cell_type":"markdown","source":"> Now the length of TRA_aa is halved.","metadata":{}},{"cell_type":"markdown","source":"# Tokenize Items","metadata":{}},{"cell_type":"code","source":"# # Initialize pre-trained model and tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# def tokenize_function(examples):\n#     outputs = tokenizer(examples, \n#                         return_tensors='pt',\n#                         truncation=True, \n#                         padding=\"max_length\", \n#                         max_length=max_length)\n#     return outputs","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:50.411798Z","iopub.execute_input":"2023-08-29T12:01:50.412912Z","iopub.status.idle":"2023-08-29T12:01:50.417755Z","shell.execute_reply.started":"2023-08-29T12:01:50.412875Z","shell.execute_reply":"2023-08-29T12:01:50.416646Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_train_tk = pd.DataFrame()\ndf_train_tk['x'] = df_train['antigen.epitope'] + '<eos>' + df_train['TRA_aa'] + '<eos>' + df_train['TRB_aa']\ndf_train_tk['y'] = df_train['tcr_affinity']\n\ndf_test_tk = pd.DataFrame()\ndf_test_tk['x'] = df_test['antigen.epitope'] + '<eos>' + df_test['TRA_aa'] + '<eos>' + df_test['TRB_aa']\ndf_test_tk['y'] = df_test['tcr_affinity']","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:50.419454Z","iopub.execute_input":"2023-08-29T12:01:50.420215Z","iopub.status.idle":"2023-08-29T12:01:50.480577Z","shell.execute_reply.started":"2023-08-29T12:01:50.420182Z","shell.execute_reply":"2023-08-29T12:01:50.479597Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## <a id='toc1_2_'></a>[Building dataset](#toc0_)","metadata":{}},{"cell_type":"code","source":"df_train.columns","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:50.483895Z","iopub.execute_input":"2023-08-29T12:01:50.484240Z","iopub.status.idle":"2023-08-29T12:01:50.490787Z","shell.execute_reply.started":"2023-08-29T12:01:50.484214Z","shell.execute_reply":"2023-08-29T12:01:50.489797Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Index(['antigen.epitope', 'TRA_aa', 'TRB_aa', 'tcr_affinity'], dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"First, we need to construct our data set. Since we are training a binary sequence classifier, we need positive and negative examples. \n\nWe simply split our data into a train/test split and place the positive and negative example sequences into separate text files.","metadata":{}},{"cell_type":"code","source":"class SequenceDataset(torch.utils.data.Dataset):\n    \"\"\"\n    What is happens here is that:\n    1. we get a dataframe with an x and y column.\n    2. encode the x column, and reshape accordingly\n    3. return encoded x column, and raw y column\n    \"\"\"\n    def __init__(self, df, plot=False):\n        self.data = df\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        \n        if plot:\n            plt.hist([len(s[0]) for s in self.data], bins=64)\n            plt.show()\n            \n        print(f\"Initialized dataset consisting of {self.data.shape[0]} sequences.\")\n        \n    def __getitem__(self, idx):\n        sequence = self.data.iloc[idx]\n        sequence_x = sequence['x']\n        encoded_sequence = self.tokenizer(sequence_x,\n                                          padding=\"max_length\",\n                                          max_length=max_length,\n                                          truncation=True, \n                                          return_tensors=\"pt\")\n\n        encoded_sequence[\"input_ids\"] = encoded_sequence[\"input_ids\"].squeeze()  # Remove the extra dimension\n        encoded_sequence[\"attention_mask\"] = encoded_sequence[\"attention_mask\"].squeeze()\n        \n        return encoded_sequence, sequence['y']\n    \n    def __len__(self):\n        return self.data.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:50.492636Z","iopub.execute_input":"2023-08-29T12:01:50.493326Z","iopub.status.idle":"2023-08-29T12:01:50.504453Z","shell.execute_reply.started":"2023-08-29T12:01:50.493291Z","shell.execute_reply":"2023-08-29T12:01:50.503346Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train = SequenceDataset(df_train_tk)\ntest = SequenceDataset(df_test_tk)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:50.506067Z","iopub.execute_input":"2023-08-29T12:01:50.506416Z","iopub.status.idle":"2023-08-29T12:01:51.058689Z","shell.execute_reply.started":"2023-08-29T12:01:50.506369Z","shell.execute_reply":"2023-08-29T12:01:51.057729Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32acb7a35ed74400847329c0a9ddf43b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d8e5ba24eb84e4ea555c3744aee3530"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06247762eb29463f82ff6a9135eaaef0"}},"metadata":{}},{"name":"stdout","text":"Initialized dataset consisting of 38528 sequences.\nInitialized dataset consisting of 1498 sequences.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Loader","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='toc1_3_'></a>[Model training](#toc0_)","metadata":{}},{"cell_type":"markdown","source":"We are using the [ESM-2 model](https://huggingface.co/docs/transformers/model_doc/esm) by Meta.\n","metadata":{}},{"cell_type":"markdown","source":"Given a trained model, we'd like to evaluate how well we are doing.\n\nMake a prediction for every example in the test set and report total accuracy (only makes sense if our test set is balanced!) and a calibration curve.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef validate(model, test_loader):\n    model.eval()\n    \n    targets = []\n    probabilities = []\n    predictions = []\n    \n    for s, l in tqdm(test_loader):\n        with torch.no_grad():\n            logits = model(**s, labels=l).logits\n            probs = torch.softmax(logits, dim=1)\n            preds = torch.argmax(logits, dim=1)\n            \n#         predictions, labels = accelerator.gather_for_metrics((\n#                 predictions, probs, preds, batch[\"label\"]\n#             ))\n        predictions.append(preds)    \n        probabilities.append(probs)\n        targets.append(l)\n            \n    targets = torch.cat(targets).cpu()\n    probabilities = torch.cat(probabilities).cpu()\n    predictions = torch.cat(predictions).cpu()\n            \n    # Visualize calibration curve\n    # prob_true, prob_pred = calibration_curve(targets, probabilities[:,1], pos_label=None, n_bins=10, strategy='uniform')\n    # sns.lineplot(x=prob_true, y=prob_pred)\n    # sns.lineplot(x=np.linspace(0,1,5), y=np.linspace(0,1,5))\n    # plt.show()\n    \n    print(\"Test accuracy:\", accuracy_score(targets, predictions))\n    print(\"Test precision:\", precision_score(targets, predictions))\n    print(\"Test recall:\", recall_score(targets, predictions))\n    cm = confusion_matrix(targets, predictions)\n    print('Confusion matrix:\\n', cm)\n\n    \n    # Reset our model to trraining mode before exiting evaluation, \n    # so we don't forget to do this later!\n    model.train()\n    \n    return(targets, probabilities, predictions)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:51.060117Z","iopub.execute_input":"2023-08-29T12:01:51.060546Z","iopub.status.idle":"2023-08-29T12:01:51.069928Z","shell.execute_reply.started":"2023-08-29T12:01:51.060512Z","shell.execute_reply":"2023-08-29T12:01:51.068978Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"> Now, we simply train our model and see how it performs.","metadata":{}},{"cell_type":"code","source":"# Reference for acceleration notebook: https://www.kaggle.com/code/muellerzr/multi-gpu-and-accelerate\n# Reference for doc of this function: https://huggingface.co/docs/accelerate/basic_tutorials/notebook\nfrom accelerate import DistributedDataParallelKwargs\n\n\n\ndef accl_training_loop(mixed_precision:str=\"fp16\", seed:int=42, batch_size:int=4):\n    set_seed(42)\n    print('seed:', seed)\n#     ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n    accelerator = Accelerator(mixed_precision=mixed_precision,\n#                               kwargs_handlers=[ddp_kwargs]\n                             ) # Change to be able to run in old kaggle gpus\n\n    with accelerator.main_process_first():\n        model = EsmForSequenceClassification.from_pretrained(model_name)\n\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n    \n    optimizer = AdamW(model.parameters(), lr=lr)\n\n    # A linear lr schedule worked well in our experiments. \n    num_training_steps = num_epochs * len(train_loader)\n    lr_scheduler = get_scheduler(\n         name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n    )\n\n    # Replacing the standard cross-entropy loss by a focal loss improved our results slightly.\n    focal = FocalLoss(gamma=0.7)\n\n    model, optimizer, train_loader, test_loader, lr_scheduler = accelerator.prepare(model, \n                                                                                    optimizer, \n                                                                                    train_loader, \n                                                                                    test_loader, \n                                                                                    lr_scheduler)\n    \n    progress_bar = tqdm(range(num_training_steps))\n    print('num_epochs:', num_epochs)\n    \n    for epoch in range(num_epochs):\n        model.train()\n\n        for idx, (s, l) in enumerate(train_loader):\n\n            logits = model(**s, labels=l).logits\n            \n            loss = focal(torch.softmax(logits, dim=1), l)\n\n            accelerator.backward(loss)\n            lr_scheduler.step()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            progress_bar.update(1)\n\n        validate(model, test_loader)\n        # To save the model afterwards to use for inference we first wait for all of the processes to be aligned\n        accelerator.wait_for_everyone() \n\n        # Then we unwrap the model from any distributed wrapping that was performed\n        model = accelerator.unwrap_model(model)\n        \n        print('PARAMETERS:')\n        print('experimental:', experimental)\n        if experimental:\n            print('experiment_on:', experiment_on)\n        print('data:', data)\n        print('model_tag:', model_tag)\n        \n    \n    \n    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    save_model_name = f\"plusnegs-3_epochs_{current_time}.pt\"\n    model.save_pretrained(save_model_name, save_function=accelerator.save)\n    return model\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:51.071572Z","iopub.execute_input":"2023-08-29T12:01:51.072175Z","iopub.status.idle":"2023-08-29T12:01:51.095721Z","shell.execute_reply.started":"2023-08-29T12:01:51.072145Z","shell.execute_reply":"2023-08-29T12:01:51.094802Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# save_model_name = f\"plusnegs-3_epochs_{current_time}.pt\"\n\n# model.save_pretrained(model, save_model_name)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:51.096916Z","iopub.execute_input":"2023-08-29T12:01:51.097651Z","iopub.status.idle":"2023-08-29T12:01:51.110607Z","shell.execute_reply.started":"2023-08-29T12:01:51.097611Z","shell.execute_reply":"2023-08-29T12:01:51.109477Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"args = (\"fp16\", 42, 4)\nmodel = notebook_launcher(accl_training_loop, args, num_processes=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:01:51.112045Z","iopub.execute_input":"2023-08-29T12:01:51.112518Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Launching training on one GPU.\nseed: 42\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/778 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"193653e7afd44342bd00eb552207982b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/136M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f66507359b44449ebaf9053653ede7a5"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/esm2_t12_35M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48160 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a996982af93424b82e169eb4f1bf2b9"}},"metadata":{}},{"name":"stdout","text":"num_epochs: 5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"   Last training::\n\n    Test accuracy: 0.9439252336448598\n    Test precision: 1.0\n    Test recall: 0.9439252336448598\n    Confusion matrix:\n     [[   0    0]\n     [  84 1414]]\n    PARAMETERS:\n    experimental: False\n    experiment_on: 1\n    data: plusnegs\n    Loading widget...\n    Test accuracy: 0.7002670226969292\n    Test precision: 1.0\n    Test recall: 0.7002670226969292\n    Confusion matrix:\n     [[   0    0]\n     [ 449 1049]]\n    PARAMETERS:\n    experimental: False\n    experiment_on: 1\n    data: plusnegs\n    Loading widget...\n    Test accuracy: 0.7917222963951935\n    Test precision: 1.0\n    Test recall: 0.7917222963951935\n    Confusion matrix:\n     [[   0    0]\n     [ 312 1186]]\n    PARAMETERS:\n    experimental: False\n    experiment_on: 1\n    data: plusnegs","metadata":{}},{"cell_type":"markdown","source":"    -------------------------------\n    experimental: False\n    data: plusnegs\n    model_tag: 35m\n \n     Test accuracy: 1.0\n    Test precision: 1.0\n    Test recall: 1.0\n    Confusion matrix:\n     [[1498]]\n\n    Test accuracy: 1.0\n    Test precision: 1.0\n    Test recall: 1.0\n    Confusion matrix:\n     [[1498]]\n    ----------------------------------\nIt appeared that I used the labels of the training set in the labels of test set. However, the x column was still test set, and I don't know how the model correctly predicted all labels with the original test x column.","metadata":{}},{"cell_type":"code","source":"# model.save_pretrained('plusnegs-3_epochs.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='toc1_4_'></a>[Lora implementation](#toc0_)","metadata":{}},{"cell_type":"markdown","source":"For larger models, you might have to do parameter efficient fine-tuning. Here, we quickly demonstrate this using a technique called [LoRA](https://arxiv.org/abs/2106.09685).","metadata":{}},{"cell_type":"code","source":"model_name = \"facebook/esm2_t12_35M_UR50D\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprint('you may need to add with accelerator.main_process_first():')\nmodel = EsmForSequenceClassification.from_pretrained(model_name)\n# model = EsmForSequenceClassification.from_pretrained(model_name).cuda()\n\n\n#\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS, inference_mode=False, r=8, lora_alpha=32, \n    lora_dropout=0.1, bias=\"none\", target_modules=[\"query\", \"value\"], \n    modules_to_save=[\"decode_head\"], # make sure to set classification heads here to save so we do train them\n)\n\nlora_model = get_peft_model(model, peft_config)\nlora_model.print_trainable_parameters()\n\noptimizer = AdamW(lora_model.parameters(), lr=lr)\nnum_training_steps = num_epochs * len(train_loader)\nlr_scheduler = get_scheduler(\n     name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n\nprogress_bar = tqdm(range(num_training_steps))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    lora_model.train()\n    \n    for s, l in train_loader:\n        \n        inputs = tokenizer(s, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=max_length).to(\"cuda\")\n        logits = lora_model(**inputs, labels=l).logits\n        \n        loss = focal(torch.softmax(logits, dim=1), l.cuda())\n        \n        loss.backward()\n        lr_scheduler.step()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        progress_bar.update(1)\n        \n    validate(lora_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results with Lora and a larger model do not get better, but maybe increasing the number of epochs would do the trick. Actually, the test accuracy keeps improving.","metadata":{}},{"cell_type":"markdown","source":"## Results:\ndata: plusnegs with only antigen column\nmodel: 8M\n\n    Test accuracy: 0.07142857142857142\n    Test precision: 0.07142857142857142\n    Test recall: 1.0\n    Confusion matrix:\n     [[   0 1872]\n     [   0  144]]\n\ndata: plusnegs with only TRA columns (1/4 sample)\nmodel: plusnegs\n\n    Test accuracy: 0.07936507936507936\n    Test precision: 0.07142857142857142\n    Test recall: 0.9907407407407407\n    Confusion matrix:\n     [[  13 1391]\n     [   1  107]]\n\ndata: plusnegs with only TRB columns (1/4 sample)\nmodel: plusnegs\n\n    Test accuracy: 0.07142857142857142\n    Test precision: 0.07142857142857142\n    Test recall: 1.0\n    Confusion matrix:\n     [[   0 1859]\n     [   0  143]]\n\n     \ndata: ds-complex with only antigen column (1/100 sample)\nmodel: 8M\n\n    Test accuracy: 0.9055211803902904\n    Test precision: 0.9307381193124368\n    Test recall: 0.8762494050452165\n    Confusion matrix:\n     [[1964  137]\n     [ 260 1841]]\ndata: ds-complex with only TRA column (1/100 sample)\nmodel: 8M\n\n    Test accuracy: 0.5\n    Test precision: 0.5\n    Test recall: 1.0\n    Confusion matrix:\n     [[   0 2101]\n     [   0 2101]]\ndata: ds-complex with only TRB column (1/100 sample)\nmodel: 8M\n\n    Test accuracy: 0.5\n    Test precision: 0.5\n    Test recall: 1.0\n    Confusion matrix:\n     [[   0 2101]\n     [   0 2101]]\n     ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}