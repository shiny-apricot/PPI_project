{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Table of contents**<a id='toc0_'></a>    \n- [Imports](#toc1_1_)    \n  - [Building dataset](#toc1_2_)    \n  - [Model training](#toc1_3_)    \n  - [Lora implementation](#toc1_4_)    \n\n<!-- vscode-jupyter-toc-config\n\tnumbering=false\n\tanchor=true\n\tflat=false\n\tminLevel=1\n\tmaxLevel=6\n\t/vscode-jupyter-toc-config -->\n<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->","metadata":{}},{"cell_type":"markdown","source":"# What is Done In This Notebook ?\n- Select the appropriate model and dataset.\n- Drop the unnecessary and NA columns.\n- Ignore the unnecessary string parts in the columns. (For ex: the last 140 characters in a column would always be the same. Then remove that part.)\n- Build the SequenceDataset class.\n- Build the evaluate function \n- Train the model using accelerate library.","metadata":{}},{"cell_type":"markdown","source":"## <a id='toc1_1_'></a>[Imports](#toc0_)","metadata":{}},{"cell_type":"code","source":"!pip install focal_loss_torch\n!pip install peft==0.4.0\n!pip install accelerate==0.22.0","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:00.784279Z","iopub.execute_input":"2023-08-28T16:06:00.784937Z","iopub.status.idle":"2023-08-28T16:06:34.604257Z","shell.execute_reply.started":"2023-08-28T16:06:00.784900Z","shell.execute_reply":"2023-08-28T16:06:34.602847Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Requirement already satisfied: focal_loss_torch in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from focal_loss_torch) (2.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from focal_loss_torch) (1.23.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->focal_loss_torch) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->focal_loss_torch) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: peft==0.4.0 in /opt/conda/lib/python3.10/site-packages (0.4.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (5.4.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (4.29.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (0.22.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (0.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.14.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0) (2023.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: accelerate==0.22.0 in /opt/conda/lib/python3.10/site-packages (0.22.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (5.4.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.22.0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.22.0) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, EsmForSequenceClassification\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report, roc_auc_score\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import auc, precision_recall_curve, average_precision_score\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom focal_loss.focal_loss import FocalLoss\nfrom torch.optim import AdamW\nfrom transformers import get_scheduler\nfrom peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n\n# Accelerate parts\nfrom accelerate import Accelerator, notebook_launcher # main interface, distributed launcher\nfrom accelerate.utils import set_seed # reproducability across devices\n\nsns.set_theme()\nsns.set_context('paper')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-08-28T16:06:34.607139Z","iopub.execute_input":"2023-08-28T16:06:34.607553Z","iopub.status.idle":"2023-08-28T16:06:34.620596Z","shell.execute_reply.started":"2023-08-28T16:06:34.607511Z","shell.execute_reply":"2023-08-28T16:06:34.619389Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# cuda = torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:34.623704Z","iopub.execute_input":"2023-08-28T16:06:34.624045Z","iopub.status.idle":"2023-08-28T16:06:34.640635Z","shell.execute_reply.started":"2023-08-28T16:06:34.624019Z","shell.execute_reply":"2023-08-28T16:06:34.639601Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:34.643468Z","iopub.execute_input":"2023-08-28T16:06:34.643799Z","iopub.status.idle":"2023-08-28T16:06:34.651952Z","shell.execute_reply.started":"2023-08-28T16:06:34.643767Z","shell.execute_reply":"2023-08-28T16:06:34.651003Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Parameters","metadata":{}},{"cell_type":"code","source":"experimental = False\n# if you set 'experimental' to True, then make this 1 to prevent confusion\nexperiment_on = 1 # example: 0.01\ndata = 'plusnegs'\n\nmodel_tag = '8m' # options are 8m, 35m, 150m\n\n# Feel free to play around with these hyper-parameters\nlr = 5e-5\nnum_epochs = 3\n\n# This is somewhat important, as sequences that are longer than this just get truncated to this length. \n# For very long sequences, this can render the model useless. In our experience, this generally still works fine. Notice that memory requirements grow dramatically with max_length!\n# max_length = 1028\nmax_length = 630\n\nif model_tag == '8m':\n    model_name = \"facebook/esm2_t6_8M_UR50D\"\nelif model_tag == '35m':\n    model_name = \"facebook/esm2_t12_35M_UR50D\"\nelif model_tag == '150m':\n    model_name = \"facebook/esm2_t30_150M_UR50D\"\nelse:\n    raise ValueError(\"Invalid model tag\")","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:34.653434Z","iopub.execute_input":"2023-08-28T16:06:34.654065Z","iopub.status.idle":"2023-08-28T16:06:34.664522Z","shell.execute_reply.started":"2023-08-28T16:06:34.654032Z","shell.execute_reply":"2023-08-28T16:06:34.663546Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"if data == 'plusnegs':\n    train_path = Path('/kaggle/input/plusnegswholedata/traintest_plusnegs_plm.csv')\n    test_path = Path('/kaggle/input/plusnegswholedata/indep_plusnegs_plm.csv')\n    \nif data == 'ds_complex-antigen':\n    train_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_train_only-antigen.csv')\n    test_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_test_only-antigen.csv')\nif data == 'ds_complex-TRA':\n    train_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_train_only-tra.csv')\n    test_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_test_only-tra.csv')\nif data == 'ds_complex-TRB':\n    train_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_train_only-trb.csv')\n    test_path = Path('/kaggle/input/ds-complex-single-columns/ds_complex_eos_tcrA_tcrB_peptide_linker_test_only-trb.csv')\n\nif data == 'plusnegs_antigen':\n    train_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/traintest_plusnegs_plm_only_antigen.csv')\n    test_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/indep_plusnegs_plm_only_antigen.csv')\nif data == 'plusnegs_TRA':\n    train_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/traintest_plusnegs_plm_only_TRA.csv')\n    test_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/indep_plusnegs_plm_only_TRA.csv')\nif data == 'plusnegs_TRB':\n    train_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/traintest_plusnegs_plm_only_TRB.csv')\n    test_path = Path('/kaggle/input/d/yasininal/plusnegssinglecolumn/indep_plusnegs_plm_only_TRB.csv')\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:34.666174Z","iopub.execute_input":"2023-08-28T16:06:34.666627Z","iopub.status.idle":"2023-08-28T16:06:34.677525Z","shell.execute_reply.started":"2023-08-28T16:06:34.666596Z","shell.execute_reply":"2023-08-28T16:06:34.676670Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(train_path)\ndf_test = pd.read_csv(test_path)\ndisplay( df_train.head(1))\n\n# drop unnecessary columns\ndf_train = df_train.drop(columns=['COMPLEX_ID'])\ndf_test = df_test.drop(columns=['COMPLEX_ID'])\ndisplay( df_train.head(1) )\nprint( df_train.shape )\n\n# drop null columns\nprint('after drop null columns')\ndf_train = df_train.dropna()\ndf_test = df_test.dropna()\nprint(df_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:34.679150Z","iopub.execute_input":"2023-08-28T16:06:34.679539Z","iopub.status.idle":"2023-08-28T16:06:34.996296Z","shell.execute_reply.started":"2023-08-28T16:06:34.679506Z","shell.execute_reply":"2023-08-28T16:06:34.995228Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"                   COMPLEX_ID antigen.epitope  \\\n0  pos_vdj_plm_train_tcr_ep_1     KAFSPEVIPMF   \n\n                                              TRA_aa  \\\n0  MLLLLVPVLEVIFTLGGTRAQSVTQLGSHVSVSEGALVLLRCNYSS...   \n\n                                              TRB_aa  tcr_affinity  \n0  MSNQVLCCVVLCFLGANTVDGGITQSPKYLFRKEGQNVTLSCEQNL...             1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>COMPLEX_ID</th>\n      <th>antigen.epitope</th>\n      <th>TRA_aa</th>\n      <th>TRB_aa</th>\n      <th>tcr_affinity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pos_vdj_plm_train_tcr_ep_1</td>\n      <td>KAFSPEVIPMF</td>\n      <td>MLLLLVPVLEVIFTLGGTRAQSVTQLGSHVSVSEGALVLLRCNYSS...</td>\n      <td>MSNQVLCCVVLCFLGANTVDGGITQSPKYLFRKEGQNVTLSCEQNL...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  antigen.epitope                                             TRA_aa  \\\n0     KAFSPEVIPMF  MLLLLVPVLEVIFTLGGTRAQSVTQLGSHVSVSEGALVLLRCNYSS...   \n\n                                              TRB_aa  tcr_affinity  \n0  MSNQVLCCVVLCFLGANTVDGGITQSPKYLFRKEGQNVTLSCEQNL...             1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>antigen.epitope</th>\n      <th>TRA_aa</th>\n      <th>TRB_aa</th>\n      <th>tcr_affinity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KAFSPEVIPMF</td>\n      <td>MLLLLVPVLEVIFTLGGTRAQSVTQLGSHVSVSEGALVLLRCNYSS...</td>\n      <td>MSNQVLCCVVLCFLGANTVDGGITQSPKYLFRKEGQNVTLSCEQNL...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"(38536, 4)\nafter drop null columns\n(38528, 4)\n","output_type":"stream"}]},{"cell_type":"code","source":"if experimental:\n    df_train = df_train.sample(n=int(df_train.shape[0] * experiment_on), random_state=44).reset_index(drop=True)\nif experimental:\n    df_test = df_test.sample(n=int(df_test.shape[0] * experiment_on), random_state=44).reset_index(drop=True)\n\nprint('the shape after sampling')\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:34.998008Z","iopub.execute_input":"2023-08-28T16:06:34.998380Z","iopub.status.idle":"2023-08-28T16:06:35.007670Z","shell.execute_reply.started":"2023-08-28T16:06:34.998337Z","shell.execute_reply":"2023-08-28T16:06:35.006547Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"the shape after sampling\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"(38528, 4)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"markdown","source":"> Calculate the length of the longest string in each column. ","metadata":{}},{"cell_type":"code","source":"max_antigen_length = df_train['antigen.epitope'].apply(len).max()\nmax_TRA_length = df_train['TRA_aa'].apply(len).max()\nmax_TRB_length = df_train['TRB_aa'].apply(len).max()\n\nmax_antigen_length, max_TRA_length, max_TRB_length","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:35.009534Z","iopub.execute_input":"2023-08-28T16:06:35.009950Z","iopub.status.idle":"2023-08-28T16:06:35.080447Z","shell.execute_reply.started":"2023-08-28T16:06:35.009887Z","shell.execute_reply":"2023-08-28T16:06:35.079325Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"(13, 285, 331)"},"metadata":{}}]},{"cell_type":"code","source":"df_train['TRA_aa'].str[-140:].unique().shape","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:35.085025Z","iopub.execute_input":"2023-08-28T16:06:35.085308Z","iopub.status.idle":"2023-08-28T16:06:35.124429Z","shell.execute_reply.started":"2023-08-28T16:06:35.085283Z","shell.execute_reply":"2023-08-28T16:06:35.123336Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"(1,)"},"metadata":{}}]},{"cell_type":"markdown","source":"> The last 140 characters of TRA_aa column is **always** the same. So we will ignore them.","metadata":{}},{"cell_type":"code","source":"df_train['TRA_aa'] = df_train['TRA_aa'].str[-140:]","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:35.125936Z","iopub.execute_input":"2023-08-28T16:06:35.126389Z","iopub.status.idle":"2023-08-28T16:06:35.152015Z","shell.execute_reply.started":"2023-08-28T16:06:35.126355Z","shell.execute_reply":"2023-08-28T16:06:35.151149Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"max_antigen_length = df_train['antigen.epitope'].apply(len).max()\nmax_TRA_length = df_train['TRA_aa'].apply(len).max()\nmax_TRB_length = df_train['TRB_aa'].apply(len).max()\n\nmax_antigen_length, max_TRA_length, max_TRB_length","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:35.153808Z","iopub.execute_input":"2023-08-28T16:06:35.154430Z","iopub.status.idle":"2023-08-28T16:06:35.222208Z","shell.execute_reply.started":"2023-08-28T16:06:35.154385Z","shell.execute_reply":"2023-08-28T16:06:35.221140Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(13, 140, 331)"},"metadata":{}}]},{"cell_type":"markdown","source":"> Now the length of TRA_aa is halved.","metadata":{}},{"cell_type":"markdown","source":"# Tokenize Items","metadata":{}},{"cell_type":"code","source":"# # Initialize pre-trained model and tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# def tokenize_function(examples):\n#     outputs = tokenizer(examples, \n#                         return_tensors='pt',\n#                         truncation=True, \n#                         padding=\"max_length\", \n#                         max_length=max_length)\n#     return outputs","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:35.223708Z","iopub.execute_input":"2023-08-28T16:06:35.224076Z","iopub.status.idle":"2023-08-28T16:06:35.333088Z","shell.execute_reply.started":"2023-08-28T16:06:35.224042Z","shell.execute_reply":"2023-08-28T16:06:35.332068Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"df_train_tk = pd.DataFrame()\ndf_train_tk['x'] = df_train['antigen.epitope'] + '<eos>' + df_train['TRA_aa'] + '<eos>' + df_train['TRB_aa']\ndf_train_tk['y'] = df_train['tcr_affinity']\n\ndf_test_tk = pd.DataFrame()\ndf_test_tk['x'] = df_test['antigen.epitope'] + '<eos>' + df_test['TRA_aa'] + '<eos>' + df_test['TRB_aa']\ndf_test_tk['y'] = df_train['tcr_affinity']","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:35.336636Z","iopub.execute_input":"2023-08-28T16:06:35.336943Z","iopub.status.idle":"2023-08-28T16:06:35.394988Z","shell.execute_reply.started":"2023-08-28T16:06:35.336917Z","shell.execute_reply":"2023-08-28T16:06:35.394053Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## <a id='toc1_2_'></a>[Building dataset](#toc0_)","metadata":{}},{"cell_type":"code","source":"df_train.columns","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:35.396583Z","iopub.execute_input":"2023-08-28T16:06:35.396957Z","iopub.status.idle":"2023-08-28T16:06:35.404269Z","shell.execute_reply.started":"2023-08-28T16:06:35.396924Z","shell.execute_reply":"2023-08-28T16:06:35.403306Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"Index(['antigen.epitope', 'TRA_aa', 'TRB_aa', 'tcr_affinity'], dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"First, we need to construct our data set. Since we are training a binary sequence classifier, we need positive and negative examples. \n\nWe simply split our data into a train/test split and place the positive and negative example sequences into separate text files.","metadata":{}},{"cell_type":"code","source":"class SequenceDataset(torch.utils.data.Dataset):\n    \"\"\"\n    What is happens here is that:\n    1. we get a dataframe with an x and y column.\n    2. encode the x column, and reshape accordingly\n    3. return encoded x column, and raw y column\n    \"\"\"\n    def __init__(self, df, plot=False):\n        self.data = df\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        \n        if plot:\n            plt.hist([len(s[0]) for s in self.data], bins=64)\n            plt.show()\n            \n        print(f\"Initialized dataset consisting of {self.data.shape[0]} sequences.\")\n        \n    def __getitem__(self, idx):\n        sequence = self.data.iloc[idx]\n        sequence_x = sequence['x']\n        encoded_sequence = self.tokenizer(sequence_x,\n                                          padding=\"max_length\",\n                                          max_length=max_length,\n                                          truncation=True, \n                                          return_tensors=\"pt\")\n\n        encoded_sequence[\"input_ids\"] = encoded_sequence[\"input_ids\"].squeeze()  # Remove the extra dimension\n        encoded_sequence[\"attention_mask\"] = encoded_sequence[\"attention_mask\"].squeeze()\n        \n        return encoded_sequence, sequence['y']\n    \n    def __len__(self):\n        return self.data.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:35.406045Z","iopub.execute_input":"2023-08-28T16:06:35.406691Z","iopub.status.idle":"2023-08-28T16:06:35.417636Z","shell.execute_reply.started":"2023-08-28T16:06:35.406658Z","shell.execute_reply":"2023-08-28T16:06:35.416767Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"train = SequenceDataset(df_train_tk)\ntest = SequenceDataset(df_test_tk)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:35.419218Z","iopub.execute_input":"2023-08-28T16:06:35.419590Z","iopub.status.idle":"2023-08-28T16:06:35.644694Z","shell.execute_reply.started":"2023-08-28T16:06:35.419558Z","shell.execute_reply":"2023-08-28T16:06:35.643653Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Initialized dataset consisting of 38528 sequences.\nInitialized dataset consisting of 1498 sequences.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Loader","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='toc1_3_'></a>[Model training](#toc0_)","metadata":{}},{"cell_type":"markdown","source":"We are using the [ESM-2 model](https://huggingface.co/docs/transformers/model_doc/esm) by Meta.\n","metadata":{}},{"cell_type":"markdown","source":"Given a trained model, we'd like to evaluate how well we are doing.\n\nMake a prediction for every example in the test set and report total accuracy (only makes sense if our test set is balanced!) and a calibration curve.","metadata":{}},{"cell_type":"code","source":"def validate(model, test_loader):\n    model.eval()\n    \n    targets = []\n    probabilities = []\n    predictions = []\n    \n    for s, l in tqdm(test_loader):\n        with torch.no_grad():\n            logits = model(**s, labels=l).logits\n            probs = torch.softmax(logits, dim=1)\n            preds = torch.argmax(logits, dim=1)\n            \n#         predictions, labels = accelerator.gather_for_metrics((\n#                 predictions, probs, preds, batch[\"label\"]\n#             ))\n        predictions.append(preds)    \n        probabilities.append(probs)\n        targets.append(l)\n            \n    targets = torch.cat(targets).cpu()\n    probabilities = torch.cat(probabilities).cpu()\n    predictions = torch.cat(predictions).cpu()\n            \n    # Visualize calibration curve\n    # prob_true, prob_pred = calibration_curve(targets, probabilities[:,1], pos_label=None, n_bins=10, strategy='uniform')\n    # sns.lineplot(x=prob_true, y=prob_pred)\n    # sns.lineplot(x=np.linspace(0,1,5), y=np.linspace(0,1,5))\n    # plt.show()\n    \n    print(\"Test accuracy:\", accuracy_score(targets, predictions))\n    print(\"Test precision:\", precision_score(targets, predictions))\n    print(\"Test recall:\", recall_score(targets, predictions))\n    print('Confusion matrix:\\n', confusion_matrix(targets, predictions))\n    \n    # Reset our model to trraining mode before exiting evaluation, \n    # so we don't forget to do this later!\n    model.train()\n    \n    return(targets, probabilities, predictions)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:35.697523Z","iopub.execute_input":"2023-08-28T16:06:35.697888Z","iopub.status.idle":"2023-08-28T16:06:35.708623Z","shell.execute_reply.started":"2023-08-28T16:06:35.697838Z","shell.execute_reply":"2023-08-28T16:06:35.707429Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"> Now, we simply train our model and see how it performs.","metadata":{}},{"cell_type":"code","source":"# Reference for acceleration: https://www.kaggle.com/code/muellerzr/multi-gpu-and-accelerate\n\ndef accl_training_loop(mixed_precision:str=\"fp16\", seed:int=42, batch_size:int=64):\n    set_seed(42)\n    accelerator = Accelerator(mixed_precision=mixed_precision) # Change to be able to run in old kaggle gpus\n\n    with accelerator.main_process_first():\n        model = EsmForSequenceClassification.from_pretrained(model_name)\n\n    train_loader = torch.utils.data.DataLoader(train, batch_size=4, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test, batch_size=4, shuffle=False)\n    \n    optimizer = AdamW(model.parameters(), lr=lr)\n\n    # A linear lr schedule worked well in our experiments. \n    num_training_steps = num_epochs * len(train_loader)\n    lr_scheduler = get_scheduler(\n         name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n    )\n\n    # Replacing the standard cross-entropy loss by a focal loss improved our results slightly.\n    focal = FocalLoss(gamma=0.7)\n\n    model, optimizer, train_loader, test_loader, lr_scheduler = accelerator.prepare(model, \n                                                                                    optimizer, \n                                                                                    train_loader, \n                                                                                    test_loader, \n                                                                                    lr_scheduler)\n    \n    progress_bar = tqdm(range(num_training_steps))\n    print('num_epochs:', num_epochs)\n    \n    for epoch in range(num_epochs):\n        model.train()\n\n        for idx, (s, l) in enumerate(train_loader):\n\n            logits = model(**s, labels=l).logits\n            \n            loss = focal(torch.softmax(logits, dim=1), l)\n\n            accelerator.backward(loss)\n            lr_scheduler.step()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            progress_bar.update(1)\n\n        validate(model, test_loader)\n        # To save the model afterwards to use for inference we first wait for all of the processes to be aligned\n        accelerator.wait_for_everyone() \n\n        # Then we unwrap the model from any distributed wrapping that was performed\n        model = accelerator.unwrap_model(model)\n        \n        print('PARAMETERS:')\n        print('experimental:', experimental)\n        print('experiment_on:', experiment_on)\n        print('data:', data)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:13:09.845689Z","iopub.execute_input":"2023-08-28T16:13:09.846078Z","iopub.status.idle":"2023-08-28T16:13:09.859744Z","shell.execute_reply.started":"2023-08-28T16:13:09.846045Z","shell.execute_reply":"2023-08-28T16:13:09.858749Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"args = (\"fp16\", 42, 64)\nnotebook_launcher(accl_training_loop, args, num_processes=2)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:13:10.271259Z","iopub.execute_input":"2023-08-28T16:13:10.271626Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Launching training on one GPU.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/28896 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c529a3193fb64ab1bae434edf41b7a22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/375 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c9aa8b60bf4e238294222efcdebee4"}},"metadata":{}},{"name":"stdout","text":"Test accuracy: 0.9439252336448598\nTest precision: 1.0\nTest recall: 0.9439252336448598\nConfusion matrix:\n [[   0    0]\n [  84 1414]]\nPARAMETERS:\nexperimental: False\nexperiment_on: 1\ndata: plusnegs\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained('./lora_tra_trb_training_peptide_all_dataset_eos')","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:35.792383Z","iopub.status.idle":"2023-08-28T16:06:35.793082Z","shell.execute_reply.started":"2023-08-28T16:06:35.792819Z","shell.execute_reply":"2023-08-28T16:06:35.792841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='toc1_4_'></a>[Lora implementation](#toc0_)","metadata":{}},{"cell_type":"markdown","source":"For larger models, you might have to do parameter efficient fine-tuning. Here, we quickly demonstrate this using a technique called [LoRA](https://arxiv.org/abs/2106.09685).","metadata":{}},{"cell_type":"code","source":"model_name = \"facebook/esm2_t12_35M_UR50D\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprint('you may need to add with accelerator.main_process_first():')\nmodel = EsmForSequenceClassification.from_pretrained(model_name)\n# model = EsmForSequenceClassification.from_pretrained(model_name).cuda()\n\n\n#\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS, inference_mode=False, r=8, lora_alpha=32, \n    lora_dropout=0.1, bias=\"none\", target_modules=[\"query\", \"value\"], \n    modules_to_save=[\"decode_head\"], # make sure to set classification heads here to save so we do train them\n)\n\nlora_model = get_peft_model(model, peft_config)\nlora_model.print_trainable_parameters()\n\noptimizer = AdamW(lora_model.parameters(), lr=lr)\nnum_training_steps = num_epochs * len(train_loader)\nlr_scheduler = get_scheduler(\n     name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n\nprogress_bar = tqdm(range(num_training_steps))","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:35.794377Z","iopub.status.idle":"2023-08-28T16:06:35.795074Z","shell.execute_reply.started":"2023-08-28T16:06:35.794808Z","shell.execute_reply":"2023-08-28T16:06:35.794831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    lora_model.train()\n    \n    for s, l in train_loader:\n        \n        inputs = tokenizer(s, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=max_length).to(\"cuda\")\n        logits = lora_model(**inputs, labels=l).logits\n        \n        loss = focal(torch.softmax(logits, dim=1), l.cuda())\n        \n        loss.backward()\n        lr_scheduler.step()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        progress_bar.update(1)\n        \n    validate(lora_model)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:06:35.796303Z","iopub.status.idle":"2023-08-28T16:06:35.797009Z","shell.execute_reply.started":"2023-08-28T16:06:35.796737Z","shell.execute_reply":"2023-08-28T16:06:35.796759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results with Lora and a larger model do not get better, but maybe increasing the number of epochs would do the trick. Actually, the test accuracy keeps improving.","metadata":{}},{"cell_type":"markdown","source":"## Results:\ndata: plusnegs with only antigen column\nmodel: 8M\n\n    Test accuracy: 0.07142857142857142\n    Test precision: 0.07142857142857142\n    Test recall: 1.0\n    Confusion matrix:\n     [[   0 1872]\n     [   0  144]]\n\ndata: plusnegs with only TRA columns (1/4 sample)\nmodel: plusnegs\n\n    Test accuracy: 0.07936507936507936\n    Test precision: 0.07142857142857142\n    Test recall: 0.9907407407407407\n    Confusion matrix:\n     [[  13 1391]\n     [   1  107]]\n\ndata: plusnegs with only TRB columns (1/4 sample)\nmodel: plusnegs\n\n    Test accuracy: 0.07142857142857142\n    Test precision: 0.07142857142857142\n    Test recall: 1.0\n    Confusion matrix:\n     [[   0 1859]\n     [   0  143]]\n\n     \ndata: ds-complex with only antigen column (1/100 sample)\nmodel: 8M\n\n    Test accuracy: 0.9055211803902904\n    Test precision: 0.9307381193124368\n    Test recall: 0.8762494050452165\n    Confusion matrix:\n     [[1964  137]\n     [ 260 1841]]\ndata: ds-complex with only TRA column (1/100 sample)\nmodel: 8M\n\n    Test accuracy: 0.5\n    Test precision: 0.5\n    Test recall: 1.0\n    Confusion matrix:\n     [[   0 2101]\n     [   0 2101]]\ndata: ds-complex with only TRB column (1/100 sample)\nmodel: 8M\n\n    Test accuracy: 0.5\n    Test precision: 0.5\n    Test recall: 1.0\n    Confusion matrix:\n     [[   0 2101]\n     [   0 2101]]\n     ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}